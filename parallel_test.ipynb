{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: transformer_lens in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.4)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.22.0)\n",
      "Requirement already satisfied: line_profiler in /opt/conda/lib/python3.10/site-packages (4.1.3)\n",
      "Collecting snakeviz\n",
      "  Downloading snakeviz-2.2.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.32.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: einops>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.8.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.2.33)\n",
      "Requirement already satisfied: rich>=12.6.0 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (13.7.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (2.2.1)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (4.42.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (4.9.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.27.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.10.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.5.0)\n",
      "Requirement already satisfied: tornado>=2.0 in /opt/conda/lib/python3.10/site-packages (from snakeviz) (6.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /opt/conda/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.15.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->transformer_lens) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
      "Downloading snakeviz-2.2.0-py2.py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m171.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: snakeviz\n",
      "Successfully installed snakeviz-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformer_lens wandb plotly line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ActivationStoreParallel import ActivationsStore\n",
    "from sparse_transcoder import SparseTranscoder\n",
    "from transcoder_training_parallel import train_transcoder_on_language_model_parallel\n",
    "from transcoder_runner_parallel import language_model_transcoder_runner_parallel\n",
    "from dataclasses import dataclass\n",
    "import transformer_lens\n",
    "import torch\n",
    "import wandb\n",
    "from typing import Optional\n",
    "from tests.test_config import test_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
<<<<<<< HEAD
    "class Config1():\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\"\n",
    "    hook_transcoder_in = \"blocks.10.hook_resid_pre\"\n",
    "    hook_point = \"blocks.10.hook_resid_pre\"\n",
    "    hook_transcoder_out = \"blocks.10.attn.hook_q\"\n",
    "    target = \"blocks.10.attn.hook_q\"\n",
    "    hook_point_layer = 10\n",
    "    ln = 'blocks.10.ln1.hook_scale'\n",
    "    d_in = 768\n",
    "    d_out = 768 * 12\n",
    "    n_head = 12\n",
    "    d_head = 64\n",
    "    dataset_path = \"Skylion007/openwebtext\"\n",
    "    is_dataset_tokenized=False\n",
    "    layer = 10\n",
    "    training = True\n",
    "    attn_scores_normed = False\n",
    "    \n",
=======
    "class UnifiedConfig():\n",
    "    # Common settings\n",
    "    model_name: str = \"gpt2-small\"\n",
    "    hook_point: str = \"blocks.10.hook_resid_pre\"\n",
    "    ln: str = 'blocks.10.ln1.hook_scale'\n",
    "    hook_point_layer: int = 10\n",
    "    layer: int = 10\n",
    "    d_in: int = 768\n",
    "    d_out: int = 768\n",
    "    n_head: int = 12\n",
    "    d_head: int = 64\n",
    "    dataset_path: str = \"Skylion007/openwebtext\"\n",
    "    is_dataset_tokenized: bool = False\n",
    "    training: bool = True\n",
    "    attn_scores_normed = True\n",
>>>>>>> d725f18 (Massive code tidy up, esp transcoder training)
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor: int = 12   # TODO: NOT being used??\n",
    "    d_hidden: int = 2400\n",
    "    b_dec_init_method: str = \"mean\"\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr: float = 1e-5\n",
    "    reg_coefficient: float = 4e-6\n",
    "    lr_scheduler_name: Optional[str] = None\n",
    "    train_batch_size: int = 2048\n",
    "    context_size: int = 256\n",
    "    lr_warm_up_steps: int = 5000\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer: int = 128\n",
    "    total_training_tokens: int = 20_000 * 5_000\n",
    "    store_batch_size: int = 32\n",
    "    use_cached_activations: bool = False\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method: str = 'none'\n",
    "    feature_sampling_window: int = 1000\n",
    "    feature_reinit_scale: float = 0.2\n",
    "    resample_batches: int = 1028\n",
    "    dead_feature_window: int = 50000\n",
    "    dead_feature_threshold: float = 1e-6\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb: bool = True\n",
    "    log_final_model_to_wandb: bool = False\n",
    "    wandb_project: str = \"sparsification\"\n",
    "    wandb_entity: Optional[str] = None\n",
    "    wandb_log_frequency: int = 50\n",
    "    entity: str = \"biggs-University College London (UCL)\"\n",
    "    \n",
    "    # Misc\n",
    "    device: str = \"cuda\"\n",
    "    eps: float = 1e-7\n",
    "    seed: int = 42\n",
    "    reshape_from_heads: bool = True\n",
    "    n_checkpoints: int = 10\n",
    "    checkpoint_path: str = \"checkpoints\"\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    run_name: str = \"qk_parallel\"\n",
    "    \n",
    "    # Query-specific settings\n",
    "    hook_transcoder_in_q: str = \"blocks.10.hook_resid_pre\"\n",
    "    hook_transcoder_out_q: str = \"blocks.10.attn.hook_q\"\n",
    "    target_q: str = \"blocks.10.attn.hook_q\"\n",
    "    type_q: str = \"resid_to_queries\"\n",
    "    \n",
    "    # Key-specific settings\n",
    "    hook_transcoder_in_k: str = \"blocks.10.hook_resid_pre\"\n",
    "    hook_transcoder_out_k: str = \"blocks.10.attn.hook_k\"\n",
    "    target_k: str = \"blocks.10.attn.hook_k\"\n",
    "    type_k: str = \"resid_to_keys\"\n",
    "\n",
    "cfg = UnifiedConfig()\n",
    "cfg.run_name = f\"{cfg.d_hidden}_{cfg.reg_coefficient}_{cfg.lr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
      "Dataset is not tokenized! Updating config.\n",
      "Setting up optimizer.\n",
      "Initializing autoencoders.\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 7.964669704437256\n",
      "New distances: 5.853984832763672\n",
      "Reinitializing b_dec_out with mean of activations\n",
      "Previous distances: 18.99521827697754\n",
      "New distances: 5.90170431137085\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 7.964669704437256\n",
      "New distances: 5.853984832763672\n",
      "Reinitializing b_dec_out with mean of activations\n",
      "Previous distances: 8.853031158447266\n",
      "New distances: 4.484612941741943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Transcoders:  51%|█████     | 512/1000 [00:00<00:00, 10192.04it/s, Loss=4.955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/final_sparse_transcoder_tiny-stories-1M_resid_to_queries_256.pt\n",
      "Saved model to checkpoints/final_sparse_transcoder_tiny-stories-1M_resid_to_keys_256.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_transcoder_Q, sparse_transcoder_K = language_model_transcoder_runner_parallel(test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
      "Dataset is not tokenized! Updating config.\n",
      "Setting up optimizer.\n",
      "Initializing autoencoders.\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 7.964669704437256\n",
      "New distances: 5.853984832763672\n",
      "Reinitializing b_dec_out with mean of activations\n",
      "Previous distances: 18.99521827697754\n",
      "New distances: 5.90170431137085\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 7.964669704437256\n",
      "New distances: 5.853984832763672\n",
      "Reinitializing b_dec_out with mean of activations\n",
      "Previous distances: 8.853031158447266\n",
      "New distances: 4.484612941741943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Transcoders:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UnifiedConfig' object has no attribute 'attn_scores_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sparse_transcoder_Q, sparse_transcoder_K \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_model_transcoder_runner_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_cfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/sparse_QK/transcoder_runner_parallel.py:28\u001b[0m, in \u001b[0;36mlanguage_model_transcoder_runner_parallel\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     25\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(entity\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mentity, project\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mwandb_project, config\u001b[38;5;241m=\u001b[39mcfg, name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrun_name)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# train SAE.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m query_transcoder, key_transcoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transcoder_on_language_model_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_transcoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_transcoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivations_loader\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# save transcoder.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m path_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_transcoder\u001b[38;5;241m.\u001b[39mget_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/workspace/sparse_QK/transcoder_training_parallel.py:81\u001b[0m, in \u001b[0;36mtrain_transcoder_on_language_model_parallel\u001b[0;34m(cfg, model, query_transcoder, key_transcoder, activation_store)\u001b[0m\n\u001b[1;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m activation_store\u001b[38;5;241m.\u001b[39mnext_batch()\n\u001b[1;32m     79\u001b[0m data \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(batch posn) d_model -> batch posn d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, posn \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mcontext_size)\n\u001b[0;32m---> 81\u001b[0m true_queries, true_keys, true_scores, true_patt \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Forward transcoder passes.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m reconstr_queries_flat, feature_actsQ, mse_lossQ, reg_lossQ\u001b[38;5;241m=\u001b[39m query_transcoder(\n\u001b[1;32m     85\u001b[0m     data,\n\u001b[1;32m     86\u001b[0m     flatten_heads(true_queries)\n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32m/workspace/sparse_QK/transcoder_training_parallel.py:170\u001b[0m, in \u001b[0;36mcompute_ground_truth\u001b[0;34m(model, data, cfg)\u001b[0m\n\u001b[1;32m    168\u001b[0m true_queries \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39meinsum(model\u001b[38;5;241m.\u001b[39mW_Q[cfg\u001b[38;5;241m.\u001b[39mlayer], data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_head d_model d_head, ... d_model -> ... n_head d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mb_Q[cfg\u001b[38;5;241m.\u001b[39mlayer]\n\u001b[1;32m    169\u001b[0m true_keys \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39meinsum(model\u001b[38;5;241m.\u001b[39mW_K[cfg\u001b[38;5;241m.\u001b[39mlayer], data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_head d_model d_head, ... d_model -> ... n_head d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mb_K[cfg\u001b[38;5;241m.\u001b[39mlayer]\n\u001b[0;32m--> 170\u001b[0m true_scores \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39meinsum(true_queries, true_keys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... posn_q n_head d_head, ... posn_k n_head d_head -> ... n_head posn_q posn_k\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_scores_norm\u001b[49m\n\u001b[1;32m    171\u001b[0m true_patt \u001b[38;5;241m=\u001b[39m apply_causal_mask(true_scores)\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m true_queries, true_keys, true_scores, true_patt\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UnifiedConfig' object has no attribute 'attn_scores_norm'"
     ]
    }
   ],
   "source": [
    "sparse_transcoder_Q, sparse_transcoder_K = language_model_transcoder_runner_parallel(test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
      "Dataset is not tokenized! Updating config.\n",
      "Setting up optimizer.\n",
      "Initializing autoencoders.\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 7.964669704437256\n",
      "New distances: 5.853984832763672\n",
      "Reinitializing b_dec_out with mean of activations\n",
      "Previous distances: 18.99521827697754\n",
      "New distances: 5.90170431137085\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 7.964669704437256\n",
      "New distances: 5.853984832763672\n",
      "Reinitializing b_dec_out with mean of activations\n",
      "Previous distances: 8.853031158447266\n",
      "New distances: 4.484612941741943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Transcoders:  51%|█████     | 512/1000 [00:00<00:00, 25655.38it/s, Loss=6.074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/final_sparse_transcoder_tiny-stories-1M_resid_to_queries_256.pt\n",
      "Saved model to checkpoints/final_sparse_transcoder_tiny-stories-1M_resid_to_keys_256.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 20.1337 s\n",
       "File: /workspace/sparse_QK/ActivationStoreParallel.py\n",
       "Function: __init__ at line 16\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    16                                               def __init__(\n",
       "    17                                                   self,\n",
       "    18                                                   cfg,\n",
       "    19                                                   model: HookedTransformer,\n",
       "    20                                                   create_dataloader: bool = True,\n",
       "    21                                               ):\n",
       "    22         1       2550.0   2550.0      0.0          self.cfg = cfg\n",
       "    23         1        490.0    490.0      0.0          self.model = model\n",
       "    24         1        1e+10    1e+10     64.2          self.dataset = load_dataset(cfg.dataset_path, split=\"train\", streaming=True, cache_dir=\"./cache\", keep_in_memory=True)\n",
       "    25         1       2850.0   2850.0      0.0          self.iterable_dataset = iter(self.dataset)\n",
       "    26                                           \n",
       "    27                                                   # check if it's tokenized\n",
       "    28         1  287838654.0    3e+08      1.4          if \"tokens\" in next(self.iterable_dataset).keys():\n",
       "    29                                                       self.cfg.is_dataset_tokenized = True\n",
       "    30                                                       print(\"Dataset is tokenized! Updating config.\")\n",
       "    31         1     478330.0 478330.0      0.0          elif \"text\" in next(self.iterable_dataset).keys():\n",
       "    32         1       2660.0   2660.0      0.0              self.cfg.is_dataset_tokenized = False\n",
       "    33         1     108395.0 108395.0      0.0              print(\"Dataset is not tokenized! Updating config.\")\n",
       "    34                                           \n",
       "    35         1       1260.0   1260.0      0.0          if self.cfg.use_cached_activations:\n",
       "    36                                                       # Sanity check: does the cache directory exist?\n",
       "    37                                                       assert os.path.exists(\n",
       "    38                                                           self.cfg.cached_activations_path\n",
       "    39                                                       ), f\"Cache directory {self.cfg.cached_activations_path} does not exist. Consider double-checking your dataset, model, and hook names.\"\n",
       "    40                                           \n",
       "    41                                                       self.next_cache_idx = 0  # which file to open next\n",
       "    42                                                       self.next_idx_within_buffer = 0  # where to start reading from in that file\n",
       "    43                                           \n",
       "    44                                                       # Check that we have enough data on disk\n",
       "    45                                                       first_buffer = torch.load(f\"{self.cfg.cached_activations_path}/0.pt\")\n",
       "    46                                                       buffer_size_on_disk = first_buffer.shape[0]\n",
       "    47                                                       n_buffers_on_disk = len(os.listdir(self.cfg.cached_activations_path))\n",
       "    48                                                       # Note: we're assuming all files have the same number of tokens\n",
       "    49                                                       # (which seems reasonable imo since that's what our script does)\n",
       "    50                                                       n_activations_on_disk = buffer_size_on_disk * n_buffers_on_disk\n",
       "    51                                                       assert (\n",
       "    52                                                           n_activations_on_disk > self.cfg.total_training_tokens\n",
       "    53                                                       ), f\"Only {n_activations_on_disk/1e6:.1f}M activations on disk, but cfg.total_training_tokens is {self.cfg.total_training_tokens/1e6:.1f}M.\"\n",
       "    54                                           \n",
       "    55                                                       # TODO add support for \"mixed loading\" (ie use cache until you run out, then switch over to streaming from HF)\n",
       "    56                                           \n",
       "    57         1        250.0    250.0      0.0          if create_dataloader:\n",
       "    58                                                       # fill buffer half a buffer, so we can mix it with a new buffer\n",
       "    59         1 3438657590.0    3e+09     17.1              self.storage_buffer = self.get_buffer(self.cfg.n_batches_in_buffer // 2)\n",
       "    60         1 3472605732.0    3e+09     17.2              self.dataloader = self.get_data_loader()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f ActivationsStore.__init__ language_model_transcoder_runner_parallel(test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sparse_transcoder_tiny-stories-1M_resid_to_keys_256'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_transcoder_K.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = SparseTranscoder.load_from_pretrained(\"checkpoints/final_sparse_transcoder_gpt2-small_resid_to_keys_2400.pt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0274, -0.0142,  0.0207,  ...,  0.0160,  0.0216, -0.0334],\n",
       "        [ 0.0333, -0.0227, -0.0014,  ...,  0.0028, -0.0333, -0.0182],\n",
       "        [ 0.0353, -0.0476, -0.0202,  ...,  0.0003, -0.0036, -0.0372],\n",
       "        ...,\n",
       "        [-0.0498,  0.0323, -0.0045,  ..., -0.0020,  0.0064, -0.0032],\n",
       "        [ 0.0111, -0.0008,  0.0419,  ...,  0.0256,  0.0485,  0.0258],\n",
       "        [ 0.0077, -0.0356,  0.0428,  ...,  0.0197,  0.0268,  0.0358]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.W_enc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
